{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goodreads_books.json', 'goodreads_books.json.gz', 'goodreads_interactions_20k_users.json', 'goodreads_interactions_20k_users.json.gz', 'goodreads_interactions_20k_users_books.json.gz', 'goodreads_interactions_20k_users_book_ids.csv', 'goodreads_interactions_dedup.json.gz', 'smaller_books.json.gz']\n"
     ]
    }
   ],
   "source": [
    "DIR = './data'\n",
    "print(os.listdir(DIR))\n",
    "USE_SMALLSET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "    book_id\n",
      "0  12479382\n",
      "1  25937839\n",
      "2    544595\n",
      "3   2059529\n",
      "4   6478867\n",
      "book_id    204839\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(os.curdir)\n",
    "df = pd.read_csv(os.path.join(DIR, \"goodreads_interactions_20k_users_book_ids.csv\"))\n",
    "print(df.head())\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smaller_books(file_name, book_id_df, count):\n",
    "    data = []\n",
    "    # convert book ids from dataframe to set\n",
    "    book_ids = book_id_df[\"book_id\"].values.tolist()\n",
    "    book_ids = [str(x) for x in book_ids]\n",
    "    # open gzip and read json lines\n",
    "    outfile_name = os.path.join(DIR,'smaller_books_'+str(count)+'.json.gz')\n",
    "    with gzip.open(file_name, 'rt') as fin, gzip.open(outfile_name, 'wt') as fout:\n",
    "        linecount = 0\n",
    "        for line in fin:\n",
    "            # convert json line to dictionary\n",
    "            book = json.loads(line)\n",
    "            fout.write(line)\n",
    "            linecount += 1\n",
    "            if linecount == count:\n",
    "                break\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_book_count = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_smaller_books(os.path.join(DIR, \"goodreads_books.json.gz\"), df, smaller_book_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books(file_name, book_id_df, count = -1, max_iter = -1):\n",
    "    data = []\n",
    "    # convert book ids from dataframe to set\n",
    "    book_ids = book_id_df[\"book_id\"].values.tolist()\n",
    "    book_ids = [str(x) for x in book_ids]\n",
    "    # open gzip and read json lines\n",
    "    with gzip.open(file_name, 'rt') as f:\n",
    "        iter = 0\n",
    "        for line in f:\n",
    "            iter += 1\n",
    "            # convert json line to dictionary\n",
    "            book = json.loads(line)\n",
    "            # check if book id is in the set\n",
    "            id = (book['book_id'])\n",
    "            if (book['book_id']) in book_ids:\n",
    "                count -= 1\n",
    "                data.append(book)\n",
    "            if count == 0:\n",
    "                break\n",
    "            if(iter == max_iter):\n",
    "                break\n",
    "        if(iter % 1000 == 0):\n",
    "            print(iter)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data\\\\smaller_books_100000.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     infile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoodreads_books.json.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m books \u001b[38;5;241m=\u001b[39m \u001b[43mload_books\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of books: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(books))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(books[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m, in \u001b[0;36mload_books\u001b[1;34m(file_name, book_id_df, count, max_iter)\u001b[0m\n\u001b[0;32m      5\u001b[0m book_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m book_ids]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# open gzip and read json lines\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\agoel\\.conda\\envs\\cs229Proj\\Lib\\gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[1;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[1;32mc:\\Users\\agoel\\.conda\\envs\\cs229Proj\\Lib\\gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, mode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data\\\\smaller_books_100000.json.gz'"
     ]
    }
   ],
   "source": [
    "if(USE_SMALLSET):\n",
    "    infile = 'smaller_books_'+str(smaller_book_count)+'.json.gz'\n",
    "else:\n",
    "    infile = 'goodreads_books.json.gz'\n",
    "\n",
    "books = load_books(os.path.join(DIR, infile), df, -1)\n",
    "print(\"Number of books: \", len(books))\n",
    "print(books[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(books)\n",
    "df.to_json(os.path.join(DIR, \"goodreads_interactions_20k_users_books.json.gz\"), orient=\"records\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings for obtained books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4474, 0.4698, 0.3909],\n",
      "        [0.5437, 0.8783, 0.3055],\n",
      "        [0.5091, 0.3499, 0.4088],\n",
      "        [0.6373, 0.3073, 0.6229],\n",
      "        [0.6841, 0.4462, 0.5326]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b783c9211b42299b5813465e90c87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agoel\\.conda\\envs\\cs229Proj\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\agoel\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[-0.34174502, -0.50928676, 0.1366887, -0.6276...\n",
      "1    [[0.0080082845, -0.085280105, 0.23509698, -0.1...\n",
      "2    [[-0.0092276335, -0.22185859, -0.34482706, 0.0...\n",
      "3    [[-0.11977858, 0.041211575, -0.19592418, -0.13...\n",
      "4    [[-0.31632385, -0.1351991, 0.3966409, -0.03750...\n",
      "Name: bert_embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Apply the function to the 'description' column\n",
    "df['bert_embeddings'] = df['description'].apply(lambda x: get_bert_embeddings(x) if pd.notnull(x) else None)\n",
    "\n",
    "print(df['bert_embeddings'].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
